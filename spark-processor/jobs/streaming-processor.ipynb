{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951002ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pbspark import MessageConverter\n",
    "from driver_fatigue_pb2 import FatigueDetection\n",
    "from seat_detection_pb2 import SeatDetection\n",
    "from bus_location_pb2 import BusLocation\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jarsPackages = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,com.datastax.spark:spark-cassandra-connector_2.12:3.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dac3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName('Spark Structured Streaming from Sensor EV-BUS') \\\n",
    "                    .config(\"spark.jars.packages\", jarsPackages) \\\n",
    "                    .config(\"spark.cassandra.connection.host\", \"10.184.0.2\") \\\n",
    "                    .config('spark.cassandra.auth.username','cassandra') \\\n",
    "                    .config('spark.cassandra.auth.password','cassandra') \\\n",
    "                    .config('spark.cassandra.connection.keepAliveMS','3600000') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0dbac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToConsole(ds,OutputMode):\n",
    "    return ds.writeStream \\\n",
    "            .outputMode(OutputMode) \\\n",
    "            .format('console') \\\n",
    "            .option('truncate', False) \\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaStream = spark.readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"103.146.34.72:9094\") \\\n",
    "      .option(\"subscribe\", 'driver_fatigue_detection,seat_detection,bus_location') \\\n",
    "      .option(\"includeHeaders\", \"true\") \\\n",
    "      .load() \\\n",
    "      .selectExpr(\"value\", \"headers\",'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f35441",
   "metadata": {},
   "outputs": [],
   "source": [
    "topicBusLocation = kafkaStream.select(col('headers'),col('value')).where(expr('headers')[0]['value'].cast('string')=='bus_location')\n",
    "topicFatigueDriver = kafkaStream.select(col('headers'),col('value')).where(expr('headers')[0]['value'].cast('string')==\"driver_fatigue_detection\")\n",
    "topicSeatDetection = kafkaStream.select(col('headers'),col('value')).where(expr('headers')[0]['value'].cast('string')==\"seat_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85adb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsedRawData(df, proto):\n",
    "    # Parsed Protobuf encryption\n",
    "    mc = MessageConverter()\n",
    "    parsedData = df.withColumn('parsed', mc.from_protobuf('value', proto)) \\\n",
    "        .withColumn('topic', expr('headers')[0]['value'].cast('string')) \\\n",
    "        .withColumn('bus_id', expr('headers')[2]['value'].cast('string')) \\\n",
    "        .select('topic','bus_id', 'parsed.*') \\\n",
    "        .withColumn('timestamp', to_timestamp(col('timestamp') / 1000))\n",
    "\n",
    "    return parsedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d5b08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DriverFatigueRaw = parsedRawData(topicFatigueDriver,FatigueDetection)\n",
    "SeatDetectionRaw = parsedRawData(topicSeatDetection,SeatDetection)\n",
    "BusLocationRaw = parsedRawData(topicBusLocation,BusLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbe784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"topic_bus_location\": \"bus_location\",\n",
    "\"topic_driver_fatigue\": \"driver_fatigue_detection\",\n",
    "\"topic_seat_detection\": \"seat_detection\",\n",
    "\"hadoop_job_path\": \"hdfs://namenode:9000/user/parallels/job\",\n",
    "\"hadoop_checkpoint_path\": \"hdfs://namenode:9000/user/parallels/checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToHDFS(ds,topic):\n",
    "    return ds \\\n",
    "        .writeStream \\\n",
    "        .outputMode('append') \\\n",
    "        .option('path', \"hdfs://192.168.193.64:9000/user/parallels/job/\") \\\n",
    "        .option('checkpointLocation', f\"hdfs://192.168.193.64:9000/user/parallels/checkpoint/{topic}/\") \\\n",
    "        .partitionBy('topic','bus_id') \\\n",
    "        .trigger(processingTime='10 seconds') \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cefe14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DriverFatigueDs = writeToHDFS(DriverFatigueRaw,'driver_fatigue_detection')\n",
    "SeatDetectionDs = writeToHDFS(SeatDetectionRaw,'seat_detection')\n",
    "BusLocationDs = writeToHDFS(BusLocationRaw,'bus_location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53026f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeToConsole(DriverFatigueRaw,'append')\n",
    "writeToConsole(BusLocationRaw,'append')\n",
    "writeToConsole(SeatDetectionRaw,'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forEachBatchAction(batch_df,batch_id):\n",
    "        return batch_df.write \\\n",
    "                .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "                .options(table='raw_data_bus_location', keyspace=\"busev\") \\\n",
    "                .mode('append')\\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc6501",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassandra = BusLocationRaw.writeStream \\\n",
    "            .trigger(processingTime='5 seconds')\\\n",
    "            .foreachBatch(forEachBatchAction) \\\n",
    "            .outputMode('update')\\\n",
    "            .start() \\\n",
    "            .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937e923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
